{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kvl4jeXLpIxn"
   },
   "source": [
    "# Как внутри работет линейная регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MlO1E37SpIxq"
   },
   "source": [
    "## ЧТО делает линейная регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "51ewUZSGpIxs"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zjlXAmktpIxx"
   },
   "source": [
    "Давайте положим в X какие-то трехмерные векторы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LI7eZK5DpIxz"
   },
   "outputs": [],
   "source": [
    "X = np.array([[1, 1, 7],\n",
    "              [1, 2, 6],\n",
    "              [2, 2, 10],\n",
    "              [2, 3, 11],\n",
    "              [10, 5, 2],\n",
    "              [14, 3, 6],\n",
    "              [20, 3, 50]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EOWz5ZU3pIx2"
   },
   "source": [
    "Построим $y$ по такой формуле:\n",
    "\n",
    "$$y = 100x - 200y + 70z + 35 + \\varepsilon$$\n",
    "\n",
    "Где $\\varepsilon$ - это какой-то шум с нормальным распределением с дисперсией 10, чтобы линейная формула не была уж совсем точной."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EI5lXs7epIx3",
    "outputId": "288dc585-d4cb-4458-8079-afb7571d4241"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 436.69662735,  152.9247788 ,  539.85356084,  405.60060537,\n",
       "        181.82416597, 1256.83684846, 4938.61532051])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.dot(X, np.array([100, -200, 70])) + 35 + np.random.normal(0, 10, len(X))\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "buHcD3bepIx8"
   },
   "source": [
    "Теперь обучим на этих данных линейную регрессию и посмотрим, сможет ли она восстановить параметры модели $100, 200, 70, 35$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "npUueCqCpIx8",
    "outputId": "b9bdc89e-5168-42ca-e799-f3a4c14c87a3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ttp6iWV7pIx_",
    "outputId": "47b23946-b1bf-4f0b-a6a9-3c65b7138de0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 100.14369851, -200.94383955,   69.9380973 ])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefs = model.coef_\n",
    "coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4IoH2Hf-pIyB",
    "outputId": "ab58faff-a62e-43d3-c76b-55fb2cbe8eb3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41.25642604338168"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const = model.intercept_\n",
    "const"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KTmTk1DDpIyD"
   },
   "source": [
    "И внезапно линейная регрессия как-то примерно угадала наши коэффициенты! Примерно, потому что мы добавили шум.\n",
    "\n",
    "Как она это делает? Сейчас узнаем."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6UB7RwvPpIyD"
   },
   "source": [
    "## КАК работает линейная регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QrPtVtm7pIyE"
   },
   "source": [
    "По сути мы хотим подобрать числа $a_0, a_1, a_2, a_3$ для вот такой модели:\n",
    "\n",
    "$$f(x_1, x_2, x_3) = a_0 + a_1 x_1 + a_2 x_2 + a_3 x_3$$\n",
    "\n",
    "Мы хотим подобрать их так, чтобы функция потерь на наших данных была минимальна. В LinearRegression используют функцию потерь MSE - сумму квадратов отклонений от настоящего значения.\n",
    "\n",
    "То есть задача такая:\n",
    "\n",
    "$$\\sum\\limits_{i=1}^{n}(f(x_{i1}, x_{i2}, x_{i3}) - y_i)^2 \\rightarrow \\min$$\n",
    "\n",
    "\n",
    "$$\\sum\\limits_{i=1}^{n}(a_0 + a_1 x_{i1} + a_2 x_{i2} + a_3 x_{i3} - y_i)^2 \\rightarrow \\min$$\n",
    "\n",
    "Где n - это количество входных данных. Давайте рассмотрим эту сумму как функцию от 4 переменных $a_0, a_1, a_2, a_3$, которую нам нужно минимизировать. А числа $x_{ij}$ и $y_i$, получается, будут обычными константами.\n",
    "\n",
    "$$MSE(a_0, a_1, a_2, a_3) \\rightarrow \\min$$\n",
    "\n",
    "Давайте посчитаем частную производную по каждой координате.\n",
    "\n",
    "Начнем с координаты $a_1$.\n",
    "\n",
    "$$MSE'_{a_1}(a_0, a_1, a_2, a_3) = \\sum\\limits_{i=1}^{n}((a_0 + a_1 x_{i1} + a_2 x_{i2} + a_3 x_{i3} - y_i)^2)'_{a_1}=$$\n",
    "\n",
    "Раскрываем квадрат, вынося отдельно члены, которые делятся на $a_1^2$, $a_1$ и $1$.\n",
    "\n",
    "$$= \\sum\\limits_{i=1}^{n}(x_{i1}^2a_1^2 + 2x_{i1}(a_0 + a_2 x_{i2} + a_3 x_{i3} - y_i)a_1 + (a_0 + a_2 x_{i2} + a_3 x_{i3} - y_i)^2)'_{a_1}=$$\n",
    "\n",
    "Считаем производную, одна из скобок при этом обнуляется:\n",
    "\n",
    "$$= \\sum\\limits_{i=1}^{n}(2x_{i1}^2a_1 + 2x_{i1}(a_0 + a_2 x_{i2} + a_3 x_{i3} - y_i))=$$\n",
    "\n",
    "Теперь вынесем $2$ и $x_{i1}$\n",
    "\n",
    "$$= 2\\sum\\limits_{i=1}^{n}(x_{i1}a_1 + a_0 + a_2 x_{i2} + a_3 x_{i3} - y_i)x_{i1}=$$\n",
    "\n",
    "Заметим, что в скобках получилось очень простое выражение!\n",
    "\n",
    "$$= 2\\sum\\limits_{i=1}^{n}(f(x_{i1}, x_{i2}, x_{i3}) - y_i)x_{i1}$$\n",
    "\n",
    "Давайте приравняем все 4 производные (по $a_0, a_1, a_2, a_3$) нулю, тогда:\n",
    "\n",
    "$$\\sum\\limits_{i=1}^{n}(a_0 + a_1 x_{i1} + a_2 x_{i2} + a_3 x_{i3} - y_i) = 0$$\n",
    "$$\\sum\\limits_{i=1}^{n}(a_0 + a_1 x_{i1} + a_2 x_{i2} + a_3 x_{i3} - y_i)x_{i1} = 0$$\n",
    "$$\\sum\\limits_{i=1}^{n}(a_0 + a_1 x_{i1} + a_2 x_{i2} + a_3 x_{i3} - y_i)x_{i2} = 0$$\n",
    "$$\\sum\\limits_{i=1}^{n}(a_0 + a_1 x_{i1} + a_2 x_{i2} + a_3 x_{i3} - y_i)x_{i3} = 0$$\n",
    "\n",
    "Давайте сгруппируем все выражения по $a_0, a_1, a_2, a_3$:\n",
    "\n",
    "$$na_0 + (\\sum\\limits_{i=1}^{n}x_{i1})a_1 + (\\sum\\limits_{i=1}^{n}x_{i2})a_2 + (\\sum\\limits_{i=1}^{n}x_{i3})a_3= \\sum\\limits_{i=1}^{n}y_i$$\n",
    "\n",
    "$$(\\sum\\limits_{i=1}^{n}x_{i1})a_0 + (\\sum\\limits_{i=1}^{n}x_{i1}^2)a_1 + (\\sum\\limits_{i=1}^{n}x_{i1}x_{i2})a_2 + (\\sum\\limits_{i=1}^{n}x_{i1}x_{i3})a_3= \\sum\\limits_{i=1}^{n}x_{i1}y_i$$\n",
    "\n",
    "$$(\\sum\\limits_{i=1}^{n}x_{i2})a_0 + (\\sum\\limits_{i=1}^{n}x_{i1}x_{i2})a_1 + (\\sum\\limits_{i=1}^{n}x_{i2}^2)a_2 + (\\sum\\limits_{i=1}^{n}x_{i2}x_{i3})a_3= \\sum\\limits_{i=1}^{n}x_{i2}y_i$$\n",
    "\n",
    "$$(\\sum\\limits_{i=1}^{n}x_{i3})a_0 + (\\sum\\limits_{i=1}^{n}x_{i1}x_{i3})a_1 + (\\sum\\limits_{i=1}^{n}x_{i2}x_{i3})a_2 + (\\sum\\limits_{i=1}^{n}x_{i3}^2)a_3= \\sum\\limits_{i=1}^{n}x_{i3}y_i$$\n",
    "\n",
    "Ура, мы получили красивую симметричную систему уравнения, 4 уравнения, 4 неизвестных. Если определитель матрицы коэффициентов не равен нулю, то у него есть ровно одно решение, и его мы умеем находить (методом Гаусса, например). Если определитель вдруг стал равен нулю, то решений либо 0, либо бесконечно.\n",
    "\n",
    "У непрерывно-дифференцируемой функции, которая при стремлении по каждой координате к плюс или минус бесконечности сама стремится к плюс бесконечности, всегда существует глобальный минимум. В точке глобального минимума все производные как раз равны нулю. Следовательно, существует всегда хотя бы одно решение, и мы его найдем."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "how_works_LinearRegression.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
