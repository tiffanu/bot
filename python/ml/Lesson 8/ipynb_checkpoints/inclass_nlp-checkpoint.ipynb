{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обработка текстов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Предобработка текстов "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Правильная предобработка текста позволяет добиться:\n",
    "* улучшения получаемых результатов\n",
    "* ускорения экспериментов\n",
    "* воспроизводимости экспериментов\n",
    "* удобной интерпретации и презентации результатов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наивные методы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = ''' «Тинькофф Банк» — российский коммерческий банк, сфокусированный полностью на дистанционном обслуживании\n",
    ", не имеющий розничных отделений. Штаб-квартира банка расположена в Москве.'''\n",
    "sent.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent.split(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Регулярные выражение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Регулярное выражение — это последовательность символов, используемая для поиска и замены текста в строке или файле"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* поиска в строке;\n",
    "* разбиения строки на подстроки;\n",
    "* замены части строки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='CheatSheet.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = re.match(r'Tinkoff', ' junior Tinkoff the best')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = re.search(r'Tinkoff', 'Tinkoff Junior the Best Team Tinkoff Bank')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = re.findall(r'Tinkoff', 'Tinkoff Junior the Best Team Tinkoff Bank')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = re.split(r'the', 'Tinkoff Junior the Best Team Tinkoff Bank')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = re.sub(r'Bank', 'World', 'Tinkoff Junior the Best Team Tinkoff Bank')\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "можно создавать паттерны"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile('Tinkoff')\n",
    "result = pattern.findall('Tinkoff Junior the Best Team Tinkoff Bank')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = 'asdf fjdk;afed,fjek,asdf,foo' \n",
    "result = re.split(r'[;,\\s]', line)\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Оставить в тексте только русские буквы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "sent = '''Ночь закрытых дверей!!! Штаб-квартира Tinkoff.ru 27 ноября'''\n",
    "expr = \n",
    "parser=re.compile(expr)\n",
    "tmp_string = re.sub(expr, '', sent)\n",
    "print(tmp_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.sub(r'\\s+',r' ',tmp_string )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Извлечь домены, заменить все домены на tinkoff.ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = re.findall(r'@(\\w+).(\\w+)', 'abc.test@gmail.com, xyz@test.in, test.first@analyticsvidhya.com, first.test@rest.biz')\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Больше примеров регулярных выражений: https://regex101.com/r/nG1gU7/27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Спелл чекеры - проверка правописания"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Левенштейн**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Расстояние Левенштейна (также редакционное расстояние или дистанция редактирования) между двумя строками в теории информации и компьютерной лингвистике — это минимальное количество операций вставки одного символа, удаления одного символа и замены одного символа на другой, необходимых для превращения одной строки в другую."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pypi.org/project/python-Levenshtein/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Levenshtein.distance('Банк', 'ит-компания')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Уменьшение словаря\n",
    "* Плохие слова:\n",
    "* Слишком частые \n",
    "  * русский язык: и, но, я, ты, ... \n",
    "  * английский язык: a, the, I, ... \n",
    "  * специфичные для коллекции: \"сообщать\" в новостях\n",
    "* Слишком редкие\n",
    "* Стоп-слова \n",
    "  *Предлоги, междометия, частицы, цифры"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NLTK**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.nltk.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "sw_eng = set(stopwords.words('english'))\n",
    "list(sw_eng)[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "sw_ru = set(stopwords.words('russian'))\n",
    "len(sw_ru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'Наступило молчание. Графиня глядела на гостью, приятно улыбаясь, впрочем, не скрывая \\\n",
    "того, что не огорчится теперь нисколько, если гостья поднимется и уедет.\\\n",
    "Дочь гостьи уже оправляла платье, вопросительно глядя на мать, как вдруг из\\\n",
    "соседней комнаты послышался бег к двери нескольких мужских и женских ног,\\\n",
    "грохот зацепленного и поваленного стула, и в комнату вбежала тринадцатилетняя девочка,\\\n",
    "запахнув что-то короткою кисейною юбкою, и остановилась посередине комнаты. Очевидно было,\\\n",
    "она нечаянно, с нерассчитанного бега, заскочила так далеко. В дверях в ту же минуту показались\\\n",
    "студент с малиновым воротником, гвардейский офицер, пятнадцатилетняя девочка и толстый румяный\\\n",
    "мальчик в детской курточке.'\n",
    "\n",
    "clean_sent = ' '.join([word for word in sent.split() if not word in sw_ru])\n",
    "print('До {} слов'.format(len(sent.split())))\n",
    "print('После {} слов'.format(len(clean_sent.split())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'быть или не быть'\n",
    "clean_sent = ' '.join([word for word in sent.split() if not word in sw_ru])\n",
    "clean_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Токенизация\n",
    "разделение на токены, элементарные единицы текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "light_string = 'Привет, какая у меня полная сумма задолженности по кредитной карте'\n",
    "light_string.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_string = 'Ой, у вас несколько кредитных карт, выберите, пожалуйста, одну и введите ее номер'\n",
    "hard_string.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "hard_string = 'Привет! Ты видел мр.Смита сегодня утром?'\n",
    "expr = r'[^(\\w.\\w)\\w\\s]'\n",
    "parser=re.compile(expr)\n",
    "tmp_string = parser.sub(r'', hard_string)\n",
    "print(tmp_string.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "hard_string = 'Привет. Ты видел мр.Смита сегодня утром?'\n",
    "tmp_string = re.split(r'[!.?]', hard_string)\n",
    "print(tmp_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_string = 'Привет. Ты видел мр.Смита сегодня утром?'\n",
    "exp = r'(?<!\\w\\.\\w.)(?<![А-Я][а-я]\\.)(?<=\\.|\\?)\\s'\n",
    "tmp_string = re.split(exp, hard_string)\n",
    "print(tmp_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Нормализация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Стемминг** - нормализация слов путем отбрасывания окончаний (согласно правилам, основанным на грамматике языка)\n",
    "* Стеммеры (nltk)\n",
    "    * Porter stemmer\n",
    "    * Snowball stemmer\n",
    "    * Lancaster stemmer\n",
    "    * MyStem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "sent = 'George admitted the talks happened'\n",
    "print(' '.join([stemmer.stem(word) for word in sent.split()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "sent = 'write wrote written'\n",
    "print(' '.join([stemmer.stem(word) for word in sent.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(language='russian')\n",
    "sent = 'Опрошенные считают налоги необходимыми'\n",
    "print(' '.join([stemmer.stem(word) for word in sent.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'поле пол полка полк'\n",
    "print(' '.join([stemmer.stem(word) for word in sent.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'крутой крутейший крутить'\n",
    "print(' '.join([stemmer.stem(word) for word in sent.split()]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Лемматизация** - приведение слов к начальной морфологической форме (с помощью словаря и грамматики языка)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лемматизаторы:\n",
    "* pymorphy2 (язык русский, украинский)\n",
    "* mystem3 (язык русский)\n",
    "* Wordnet Lemmatizer (NLTK, язык английский, требует POS метку)\n",
    "* Metaphraz (язык русский)\n",
    "* Coda/Cadenza (языки русский и английский)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лемматизатор на самом деле довольно сложно устроены, им нужны теги частей речи (POS).\n",
    "\n",
    "По умолчанию функция WordNetLemmatizer.lemmatize () будет считать, что это слово является существительным, если на входе не обнаружен тег POS.\n",
    "\n",
    "Сначала вам понадобится функция pos_tag, чтобы пометить предложение и использовать тег, чтобы преобразовать его в теги WordNet, а затем передать его в WordNetLemmatizer.\n",
    "\n",
    "Примечание. Лемматизация не будет работать только на одиночных словах без контекста или знании своего тега POS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import wordnet, pos_tag\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    my_switch = {\n",
    "        'J': wordnet.wordnet.ADJ,\n",
    "        'V': wordnet.wordnet.VERB,\n",
    "        'N': wordnet.wordnet.NOUN,\n",
    "        'R': wordnet.wordnet.ADV,\n",
    "    }\n",
    "    for key, item in my_switch.items():\n",
    "        if treebank_tag.startswith(key):\n",
    "            return item\n",
    "    return wordnet.wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'George admitted the talks happened'.split()\n",
    "pos_tagged = pos_tag(sent)\n",
    "print(pos_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([get_wordnet_pos(tag) for word, tag in pos_tagged])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "def my_lemmatizer(sent):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokenized_sent = sent.split()\n",
    "    pos_tagged = [(word, get_wordnet_pos(tag))\n",
    "                 for word, tag in pos_tag(tokenized_sent)]\n",
    "    return ' '.join([lemmatizer.lemmatize(word, tag)\n",
    "                    for word, tag in pos_tagged])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'George admitted the talks happened'\n",
    "my_lemmatizer(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'write wrote written'\n",
    "my_lemmatizer(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "def my_lemmatizer_ru(sent):\n",
    "    lemmatizer = pymorphy2.MorphAnalyzer()\n",
    "    tokenized_sent = sent.split()\n",
    "    return ' '.join([lemmatizer.parse(word)[0].normal_form\n",
    "                    for word in tokenized_sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'Опрошенные считают налоги необходимыми'\n",
    "my_lemmatizer_ru(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'Выйду в поле с конем'\n",
    "my_lemmatizer_ru(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'крутой крутейший крутить'\n",
    "print(my_lemmatizer_ru(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Стемминг *\n",
    "* Плохо работает для русского языка\n",
    "* Нормально работает для английского\n",
    "* Повышает качество модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Лемматизация*\n",
    "* Лучше стемминга для русского языка\n",
    "* Хорошо работает для английского языка\n",
    "* Повышает качество модели\n",
    "* Гораздо медленнее чем стемминг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Представление текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding\n",
    "\n",
    "Представление словаря в виде бинарных векторов, у которых все значения равны 0, кроме одного, отвечающего за соответствующее слово"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "corpus = ['Кредитная Дебетовая', 'Дебетовая', 'All Airlines', 'Bravo', 'All games']\n",
    "label_encoder = LabelEncoder()\n",
    "corpus_encoded = label_encoder.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "corpus_encoded = corpus_encoded.reshape(len(corpus_encoded), 1)\n",
    "corpus_onehot_encoded = onehot_encoder.fit_transform(corpus_encoded)\n",
    "print(corpus_onehot_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'This is the first text text text.',\n",
    "    'This is the second second text.',\n",
    "    'And the third one.',\n",
    "    'Is this the first text?',\n",
    "]\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "idf_vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "Y = idf_vectorizer.fit_transform(corpus)\n",
    "print(idf_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "categories = ['alt.atheism', 'soc.religion.christian',\n",
    "              'comp.graphics', 'sci.med']\n",
    "twenty_train = fetch_20newsgroups(subset='train',\n",
    "                                  categories=categories, shuffle=True, random_state=42)\n",
    "twenty_test = fetch_20newsgroups(subset='test',\n",
    "                                 categories=categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\".join(twenty_train.data[0].split(\"\\n\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(twenty_train.target_names[twenty_train.target[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(twenty_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_train.target[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_counts = count_vect.transform(twenty_test.data)\n",
    "X_test_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "svm = SGDClassifier()\n",
    "svm.fit(X_train_counts, twenty_train.target)\n",
    "predicted = svm.predict(X_test_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(twenty_test.target, predicted,\n",
    "                                    target_names=twenty_test.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Самостоятельная работа: попробовать другие преобразования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
